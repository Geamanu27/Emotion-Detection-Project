{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82f61adc",
   "metadata": {},
   "source": [
    "\n",
    "# Emotion Classification — MobileNet (TensorFlow/Keras)\n",
    "\n",
    "This notebook trains a MobileNet-based image classifier using `flow_from_directory`.\n",
    "It follows a clean, **cell-by-cell** structure so you can adapt paths and parameters easily.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a01da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %% [markdown]\n",
    "# ## 1. Environment & Version Check\n",
    "import sys, tensorflow as tf, numpy as np, PIL\n",
    "\n",
    "print(\"Python:\", sys.version)\n",
    "print(\"TensorFlow:\", tf.__version__)\n",
    "print(\"NumPy:\", np.__version__)\n",
    "print(\"Pillow:\", PIL.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a85e642",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %% [markdown]\n",
    "# ## 2. Configuration\n",
    "# Update these paths to your dataset.\n",
    "# The directory structure should be:\n",
    "# train_dir/\n",
    "#   class_a/\n",
    "#     img1.jpg, img2.jpg, ...\n",
    "#   class_b/\n",
    "#     ...\n",
    "# val_dir/\n",
    "#   class_a/\n",
    "#   class_b/\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "# >>>> EDIT THESE <<<<\n",
    "train_dir = Path(\"/path/to/fer2013/train\")       # e.g., D:/data/fer2013/train\n",
    "val_dir   = Path(\"/path/to/fer2013/validation\")  # e.g., D:/data/fer2013/validation\n",
    "\n",
    "# Image size for MobileNet-like backbones\n",
    "IMG_SIZE = (224, 224)\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 25\n",
    "SEED = 42\n",
    "\n",
    "# Choose backbone\n",
    "BACKBONE = \"MobileNetV2\"   # options: \"MobileNet\", \"MobileNetV2\"\n",
    "FREEZE_UP_TO = 0           # number of layers to freeze (0 = train all)\n",
    "LEARNING_RATE = 1e-3\n",
    "\n",
    "# Output files\n",
    "OUT_DIR = Path(\"./outputs\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "MODEL_BEST_PATH = str(OUT_DIR / \"emotion_mobilenet_best.h5\")\n",
    "MODEL_LAST_PATH = str(OUT_DIR / \"emotion_mobilenet_last.h5\")\n",
    "HISTORY_PATH    = str(OUT_DIR / \"train_history.npy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b0646f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %% [markdown]\n",
    "# ## 3. Data Loading (ImageDataGenerator / flow_from_directory)\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "assert train_dir.exists(), f\"Train directory not found: {train_dir}\"\n",
    "assert val_dir.exists(), f\"Validation directory not found: {val_dir}\"\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=30,\n",
    "    width_shift_range=0.3,\n",
    "    height_shift_range=0.3,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "val_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_gen = train_datagen.flow_from_directory(\n",
    "    str(train_dir),\n",
    "    target_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    shuffle=True,\n",
    "    seed=SEED\n",
    ")\n",
    "\n",
    "val_gen = val_datagen.flow_from_directory(\n",
    "    str(val_dir),\n",
    "    target_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "num_classes = len(train_gen.class_indices)\n",
    "print(\"Detected classes:\", train_gen.class_indices)\n",
    "print(\"num_classes =\", num_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a17267e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %% [markdown]\n",
    "# ## 4. Build Model (MobileNet / MobileNetV2 + Custom Head)\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.applications import MobileNet, MobileNetV2\n",
    "\n",
    "inputs = layers.Input(shape=(IMG_SIZE[0], IMG_SIZE[1], 3))\n",
    "\n",
    "if BACKBONE == \"MobileNet\":\n",
    "    base = MobileNet(weights='imagenet', include_top=False, input_tensor=inputs)\n",
    "elif BACKBONE == \"MobileNetV2\":\n",
    "    base = MobileNetV2(weights='imagenet', include_top=False, input_tensor=inputs)\n",
    "else:\n",
    "    raise ValueError(\"Unsupported BACKBONE. Use 'MobileNet' or 'MobileNetV2'.\")\n",
    "\n",
    "# Freeze first N layers if requested\n",
    "for i, layer in enumerate(base.layers):\n",
    "    layer.trainable = i >= FREEZE_UP_TO\n",
    "\n",
    "x = layers.GlobalAveragePooling2D()(base.output)\n",
    "x = layers.Dense(1024, activation='relu')(x)\n",
    "x = layers.Dense(1024, activation='relu')(x)\n",
    "x = layers.Dense(512, activation='relu')(x)\n",
    "outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "model = models.Model(inputs=inputs, outputs=outputs)\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "776d9702",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %% [markdown]\n",
    "# ## 5. Compile\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "model.compile(\n",
    "    loss='categorical_crossentropy',\n",
    "    optimizer=Adam(learning_rate=LEARNING_RATE),\n",
    "    metrics=['accuracy']\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe914d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %% [markdown]\n",
    "# ## 6. Callbacks\n",
    "import tensorflow as tf\n",
    "\n",
    "checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(\n",
    "    MODEL_BEST_PATH,\n",
    "    monitor='val_loss',\n",
    "    mode='min',\n",
    "    save_best_only=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "earlystop_cb = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=10,\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "reduce_lr_cb = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.2,\n",
    "    patience=5,\n",
    "    min_lr=1e-5,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "callbacks = [checkpoint_cb, earlystop_cb, reduce_lr_cb]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d1162fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %% [markdown]\n",
    "# ## 7. Train\n",
    "import math\n",
    "\n",
    "steps_per_epoch = math.ceil(train_gen.samples / BATCH_SIZE)\n",
    "validation_steps = math.ceil(val_gen.samples / BATCH_SIZE)\n",
    "\n",
    "history = model.fit(\n",
    "    train_gen,\n",
    "    epochs=EPOCHS,\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    validation_data=val_gen,\n",
    "    validation_steps=validation_steps,\n",
    "    callbacks=callbacks\n",
    ")\n",
    "\n",
    "# Save last epoch model & history\n",
    "model.save(MODEL_LAST_PATH)\n",
    "import numpy as np\n",
    "np.save(HISTORY_PATH, history.history, allow_pickle=True)\n",
    "print(\"Saved:\", MODEL_LAST_PATH, \"and training history:\", HISTORY_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6dc04c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %% [markdown]\n",
    "# ## 8. Plot Training Curves\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "hist = np.load(HISTORY_PATH, allow_pickle=True).item()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(hist['loss'], label='train_loss')\n",
    "plt.plot(hist['val_loss'], label='val_loss')\n",
    "plt.title('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(hist['accuracy'], label='train_acc')\n",
    "plt.plot(hist['val_accuracy'], label='val_acc')\n",
    "plt.title('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e58bbe9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %% [markdown]\n",
    "# ## 9. Evaluation & Sample Predictions\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "# Evaluate\n",
    "val_loss, val_acc = model.evaluate(val_gen, steps=validation_steps, verbose=1)\n",
    "print(f\"Validation — loss: {val_loss:.4f}, acc: {val_acc:.4f}\")\n",
    "\n",
    "# Predictions\n",
    "val_gen.reset()\n",
    "pred_probs = model.predict(val_gen, steps=validation_steps, verbose=1)\n",
    "y_pred = np.argmax(pred_probs, axis=1)\n",
    "y_true = val_gen.classes\n",
    "\n",
    "# Align lengths (in case of last partial batch)\n",
    "y_true = y_true[:len(y_pred)]\n",
    "\n",
    "target_names = list(val_gen.class_indices.keys())\n",
    "print(classification_report(y_true, y_pred, target_names=target_names))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d69b5156",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %% [markdown]\n",
    "# ## 10. Confusion Matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "fig = plt.figure()\n",
    "plt.imshow(cm, interpolation='nearest')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.colorbar()\n",
    "tick_marks = np.arange(len(target_names))\n",
    "plt.xticks(tick_marks, target_names, rotation=45, ha='right')\n",
    "plt.yticks(tick_marks, target_names)\n",
    "plt.tight_layout()\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f338c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %% [markdown]\n",
    "# ## 11. Single-Image Inference Helper\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from pathlib import Path\n",
    "\n",
    "def predict_image(path):\n",
    "    img = image.load_img(path, target_size=IMG_SIZE)\n",
    "    arr = image.img_to_array(img) / 255.0\n",
    "    arr = np.expand_dims(arr, axis=0)\n",
    "    preds = model.predict(arr)\n",
    "    idx = int(np.argmax(preds))\n",
    "    classes = list(train_gen.class_indices.keys())\n",
    "    return classes[idx], float(np.max(preds))\n",
    "\n",
    "# Example:\n",
    "# test_img = Path(\"/path/to/an/image.jpg\")\n",
    "# label, prob = predict_image(test_img)\n",
    "# print(label, prob)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e11a3b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %% [markdown]\n",
    "# ## 12. (Optional) Fine-Tuning: Unfreeze More Layers and Train with Lower LR\n",
    "# Run this after initial training if you want to fine-tune deeper layers.\n",
    "\n",
    "# Example: unfreeze last N layers\n",
    "N_UNFREEZE = 30  # e.g., unfreeze last 30 layers\n",
    "for layer in model.layers[-N_UNFREEZE:]:\n",
    "    layer.trainable = True\n",
    "\n",
    "# Re-compile with lower LR\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "model.compile(\n",
    "    loss='categorical_crossentropy',\n",
    "    optimizer=Adam(learning_rate=1e-4),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "history_ft = model.fit(\n",
    "    train_gen,\n",
    "    epochs=5,\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    validation_data=val_gen,\n",
    "    validation_steps=validation_steps,\n",
    "    callbacks=callbacks\n",
    ")\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
